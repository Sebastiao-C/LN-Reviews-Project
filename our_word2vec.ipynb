{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72dd0877-60e9-403c-9e35-217a57b4c030",
   "metadata": {},
   "source": [
    "# Imports and downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d5dda2-6a52-4908-8291-887157ae724e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:40.654049Z",
     "iopub.status.busy": "2023-10-27T20:23:40.653905Z",
     "iopub.status.idle": "2023-10-27T20:23:44.009672Z",
     "shell.execute_reply": "2023-10-27T20:23:44.008776Z",
     "shell.execute_reply.started": "2023-10-27T20:23:40.654033Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "import string\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4eeb425-56cd-41b4-a3f8-2b83eb4f4358",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:44.011476Z",
     "iopub.status.busy": "2023-10-27T20:23:44.011196Z",
     "iopub.status.idle": "2023-10-27T20:23:44.015111Z",
     "shell.execute_reply": "2023-10-27T20:23:44.014424Z",
     "shell.execute_reply.started": "2023-10-27T20:23:44.011461Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00bad995-43bb-4ca7-a272-9ddad29218ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:44.016118Z",
     "iopub.status.busy": "2023-10-27T20:23:44.015942Z",
     "iopub.status.idle": "2023-10-27T20:23:44.029700Z",
     "shell.execute_reply": "2023-10-27T20:23:44.028832Z",
     "shell.execute_reply.started": "2023-10-27T20:23:44.016104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c048a84-ad41-4464-9e7f-83e115d900ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:44.030881Z",
     "iopub.status.busy": "2023-10-27T20:23:44.030655Z",
     "iopub.status.idle": "2023-10-27T20:23:44.040389Z",
     "shell.execute_reply": "2023-10-27T20:23:44.039528Z",
     "shell.execute_reply.started": "2023-10-27T20:23:44.030864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d09dd0-b24e-46c0-a81e-f5343af21feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:44.041422Z",
     "iopub.status.busy": "2023-10-27T20:23:44.041232Z",
     "iopub.status.idle": "2023-10-27T20:23:44.168490Z",
     "shell.execute_reply": "2023-10-27T20:23:44.167684Z",
     "shell.execute_reply.started": "2023-10-27T20:23:44.041408Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/sebas/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/sebas/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sebas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package sentence_polarity to\n",
      "[nltk_data]     /home/sebas/nltk_data...\n",
      "[nltk_data]   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to /home/sebas/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/sebas/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"brown\")\n",
    "nltk.download(\"movie_reviews\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('sentence_polarity')\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b1e74e-3950-40b5-a866-b188a9754c56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:44.169574Z",
     "iopub.status.busy": "2023-10-27T20:23:44.169340Z",
     "iopub.status.idle": "2023-10-27T20:23:44.173064Z",
     "shell.execute_reply": "2023-10-27T20:23:44.172379Z",
     "shell.execute_reply.started": "2023-10-27T20:23:44.169560Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words as nltk_vocabulary\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.corpus import brown, movie_reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d6c3d-dc35-49ec-89dc-27f1dc0bdd3f",
   "metadata": {},
   "source": [
    "# Creating the corpus for word2vec training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "723eb920-7d4a-4184-be41-35d3987b8667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:44.175136Z",
     "iopub.status.busy": "2023-10-27T20:23:44.174847Z",
     "iopub.status.idle": "2023-10-27T20:23:44.230437Z",
     "shell.execute_reply": "2023-10-27T20:23:44.229525Z",
     "shell.execute_reply.started": "2023-10-27T20:23:44.175123Z"
    }
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "nltk_vocabulary = nltk_vocabulary.words()\n",
    "\n",
    "root = '/home/sebas/nltk_data/corpora/sentence_polarity/'\n",
    "sentence_polarity = PlaintextCorpusReader(root, \".*\\.txt\")\n",
    "\n",
    "root = '/home/sebas/nltk_data/corpora/gutenberg/'\n",
    "gutenberg_text = PlaintextCorpusReader(root, \"melville-moby_dick.txt\")\n",
    "gutenberg_text_2 = PlaintextCorpusReader(root, \"whitman-leaves.txt\")\n",
    "gutenberg_text_3 = PlaintextCorpusReader(root, \"bible-kjv.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b17cc3-8ff3-4196-b2f8-1b57afeed85e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:44.231519Z",
     "iopub.status.busy": "2023-10-27T20:23:44.231318Z",
     "iopub.status.idle": "2023-10-27T20:23:49.992556Z",
     "shell.execute_reply": "2023-10-27T20:23:49.991084Z",
     "shell.execute_reply.started": "2023-10-27T20:23:44.231502Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = list(brown.words()) \\\n",
    "                + list(movie_reviews.words()) \\\n",
    "                + list (sentence_polarity.words()) \\\n",
    "                + list(gutenberg_text.words()) \\\n",
    "                + list(gutenberg_text_2.words()) \\\n",
    "                + list(gutenberg_text_3.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "691a6a2d-7a44-443f-8516-17dd54c6201f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:49.994175Z",
     "iopub.status.busy": "2023-10-27T20:23:49.993959Z",
     "iopub.status.idle": "2023-10-27T20:23:49.999237Z",
     "shell.execute_reply": "2023-10-27T20:23:49.998314Z",
     "shell.execute_reply.started": "2023-10-27T20:23:49.994161Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4414840"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca70d01-0526-494a-999e-687417cde269",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c899798-ba83-4a52-8ac4-609bc5051ece",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:50.000638Z",
     "iopub.status.busy": "2023-10-27T20:23:50.000143Z",
     "iopub.status.idle": "2023-10-27T20:23:50.303605Z",
     "shell.execute_reply": "2023-10-27T20:23:50.302562Z",
     "shell.execute_reply.started": "2023-10-27T20:23:50.000615Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_aux = [x.lower() for x in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "780d8589-125f-41c9-a5ca-eafc7d7454db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:50.305113Z",
     "iopub.status.busy": "2023-10-27T20:23:50.304565Z",
     "iopub.status.idle": "2023-10-27T20:23:50.853548Z",
     "shell.execute_reply": "2023-10-27T20:23:50.852449Z",
     "shell.execute_reply.started": "2023-10-27T20:23:50.305087Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [x for x in corpus_aux if ((x not in stops) and (x not in string.punctuation) and (x != '\\'\\'')  and (x != '``') and (x != '--') and (not x.isnumeric()))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18680545-a860-4146-8960-8d56c3a64f19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:50.854527Z",
     "iopub.status.busy": "2023-10-27T20:23:50.854346Z",
     "iopub.status.idle": "2023-10-27T20:23:50.860598Z",
     "shell.execute_reply": "2023-10-27T20:23:50.859690Z",
     "shell.execute_reply.started": "2023-10-27T20:23:50.854514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1902441"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cb70e14-cceb-4b51-b66b-181945725f92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:50.862080Z",
     "iopub.status.busy": "2023-10-27T20:23:50.861488Z",
     "iopub.status.idle": "2023-10-27T20:23:50.881961Z",
     "shell.execute_reply": "2023-10-27T20:23:50.881235Z",
     "shell.execute_reply.started": "2023-10-27T20:23:50.862054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a81a33fe-0c4e-4028-bd26-8a6eded20c41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:50.883388Z",
     "iopub.status.busy": "2023-10-27T20:23:50.882817Z",
     "iopub.status.idle": "2023-10-27T20:23:50.895006Z",
     "shell.execute_reply": "2023-10-27T20:23:50.893967Z",
     "shell.execute_reply.started": "2023-10-27T20:23:50.883372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', \"atlanta's\", 'recent', 'primary', 'election', 'produced', 'evidence', 'irregularities', 'took', 'place', 'jury', 'said', 'term-end', 'presentments', 'city', 'executive', 'committee', 'over-all', 'charge', 'election', 'deserves', 'praise', 'thanks', 'city', 'atlanta', 'manner', 'election', 'conducted', 'september-october', 'term', 'jury', 'charged', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'investigate', 'reports', 'possible', 'irregularities', 'hard-fought', 'primary', 'mayor-nominate', 'ivan', 'allen', 'jr.', 'relative', 'handful', 'reports', 'received', 'jury', 'said', 'considering', 'widespread', 'interest', 'election', 'number', 'voters', 'size', 'city', 'jury', 'said', 'find', 'many', \"georgia's\", 'registration', 'election', 'laws', 'outmoded', 'inadequate', 'often', 'ambiguous', 'recommended', 'fulton', 'legislators', 'act', 'laws', 'studied', 'revised', 'end', 'modernizing', 'improving', 'grand', 'jury', 'commented', 'number', 'topics', 'among', 'atlanta', 'fulton', 'county', 'purchasing']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7abf01c3-07fa-4671-af49-f96924ac037a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:50.896680Z",
     "iopub.status.busy": "2023-10-27T20:23:50.896070Z",
     "iopub.status.idle": "2023-10-27T20:23:50.904647Z",
     "shell.execute_reply": "2023-10-27T20:23:50.903966Z",
     "shell.execute_reply.started": "2023-10-27T20:23:50.896653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85341c18-cafc-4618-b3c5-baf16f2a77ac",
   "metadata": {},
   "source": [
    "# Creating necessary structures for Word2Vec training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77c91d17-7d5f-4c29-8103-19736d01415a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:50.906090Z",
     "iopub.status.busy": "2023-10-27T20:23:50.905460Z",
     "iopub.status.idle": "2023-10-27T20:23:51.017978Z",
     "shell.execute_reply": "2023-10-27T20:23:51.017044Z",
     "shell.execute_reply.started": "2023-10-27T20:23:50.906072Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_series = pd.Series(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8d3f669-e2a4-4762-be52-47461082507e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:51.018949Z",
     "iopub.status.busy": "2023-10-27T20:23:51.018786Z",
     "iopub.status.idle": "2023-10-27T20:23:51.023364Z",
     "shell.execute_reply": "2023-10-27T20:23:51.022653Z",
     "shell.execute_reply.started": "2023-10-27T20:23:51.018937Z"
    }
   },
   "outputs": [],
   "source": [
    "#corpus_series.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "698262cc-f165-496e-8bd7-d5971e76f74b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:51.024788Z",
     "iopub.status.busy": "2023-10-27T20:23:51.024333Z",
     "iopub.status.idle": "2023-10-27T20:23:51.268675Z",
     "shell.execute_reply": "2023-10-27T20:23:51.267785Z",
     "shell.execute_reply.started": "2023-10-27T20:23:51.024769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one             13159\n",
       "film            11217\n",
       "shall           10507\n",
       "unto             9023\n",
       "lord             8212\n",
       "                ...  \n",
       "rilly               1\n",
       "unprocurable        1\n",
       "moire               1\n",
       "doled               1\n",
       "chrysoprasus        1\n",
       "Name: count, Length: 80357, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = corpus_series.value_counts()\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8113fcc1-e5dc-45e3-bb00-08accfc93762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:51.269801Z",
     "iopub.status.busy": "2023-10-27T20:23:51.269570Z",
     "iopub.status.idle": "2023-10-27T20:23:51.275333Z",
     "shell.execute_reply": "2023-10-27T20:23:51.274711Z",
     "shell.execute_reply.started": "2023-10-27T20:23:51.269773Z"
    }
   },
   "outputs": [],
   "source": [
    "min_freq = 20\n",
    "relevant_words = word_counts[word_counts > min_freq]\n",
    "#list(relevant_words.index)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "688a81ba-3098-4a05-bb80-60b8518c1375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:51.276316Z",
     "iopub.status.busy": "2023-10-27T20:23:51.276148Z",
     "iopub.status.idle": "2023-10-27T20:23:51.300852Z",
     "shell.execute_reply": "2023-10-27T20:23:51.300171Z",
     "shell.execute_reply.started": "2023-10-27T20:23:51.276304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11216"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = pd.DataFrame(relevant_words).reset_index()[['index']].reset_index().iloc[:,0]\n",
    "words = pd.DataFrame(relevant_words).reset_index()[['index']].reset_index().iloc[:,1]\n",
    "\n",
    "ind_to_words_dict = dict(zip(indices, words))\n",
    "words_to_ind_dict = dict(zip(words, indices))\n",
    "dict_len = len(list(words_to_ind_dict.keys()))\n",
    "dict_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63a25c41-2907-40c8-b62a-be66fc5a6326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:51.302218Z",
     "iopub.status.busy": "2023-10-27T20:23:51.301688Z",
     "iopub.status.idle": "2023-10-27T20:23:51.311163Z",
     "shell.execute_reply": "2023-10-27T20:23:51.310356Z",
     "shell.execute_reply.started": "2023-10-27T20:23:51.302197Z"
    }
   },
   "outputs": [],
   "source": [
    "#words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08bababf-2416-4979-a9e3-a73b2bdb0d94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:51.314725Z",
     "iopub.status.busy": "2023-10-27T20:23:51.314499Z",
     "iopub.status.idle": "2023-10-27T20:23:51.456393Z",
     "shell.execute_reply": "2023-10-27T20:23:51.455571Z",
     "shell.execute_reply.started": "2023-10-27T20:23:51.314710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1649424"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_words = set( words.values)\n",
    "corpus_valids = [x in set_words for x in corpus]\n",
    "sum(corpus_valids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153013b0-cd07-499b-9ee4-e2ce14f01fa8",
   "metadata": {},
   "source": [
    "# Creating the training dataset with context-target pairs and negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afa7dd98-0c7f-46a5-9e86-3362df559d04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:51.457532Z",
     "iopub.status.busy": "2023-10-27T20:23:51.457264Z",
     "iopub.status.idle": "2023-10-27T20:23:51.462568Z",
     "shell.execute_reply": "2023-10-27T20:23:51.461922Z",
     "shell.execute_reply.started": "2023-10-27T20:23:51.457497Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_context_pairs(i, words, word):#wordn2, wordn1, word, wordp1, wordp2):\n",
    "    positive_exs = list()\n",
    "    valids = list()\n",
    "\n",
    "    n_words = len(words)\n",
    "\n",
    "    for k, word_ in enumerate(words, int(-((n_words-1)/2))):\n",
    "        #if i == 0:\n",
    "        #    print(\"inside\", k)\n",
    "        if k != 0:\n",
    "            if corpus_valids[i+k]:\n",
    "                positive_exs.append(word_)\n",
    "                valids.append(k)\n",
    "            #if ((k == -1) or (k == 1)) and brown_valids[i+k]:\n",
    "                \n",
    "    return positive_exs, valids        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4323f81-49b0-49a6-8aa4-708996c4bacf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:23:51.463880Z",
     "iopub.status.busy": "2023-10-27T20:23:51.463522Z",
     "iopub.status.idle": "2023-10-27T20:29:37.060199Z",
     "shell.execute_reply": "2023-10-27T20:29:37.058318Z",
     "shell.execute_reply.started": "2023-10-27T20:23:51.463863Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500000\n",
      "1000000\n",
      "1500000\n"
     ]
    }
   ],
   "source": [
    "context_window = 5\n",
    "training_set = list()\n",
    "\n",
    "for i, word in enumerate(corpus):\n",
    "    if (i % 500000) == 0:\n",
    "        print(i)\n",
    "    if i - context_window >= 0 and i + context_window <= (len(corpus)-1):\n",
    "        if corpus_valids[i]:\n",
    "            con_words = list()\n",
    "            \n",
    "            for k in range(-context_window, context_window+1, 1):\n",
    "                #if i == 0:\n",
    "                #    print(\"outside\", k)\n",
    "                con_words.append(corpus[i+k])\n",
    "            positives, valids = get_context_pairs(i, con_words, word)\n",
    "            \n",
    "            num_valids = len(valids)\n",
    "            forbidden_ints = list()\n",
    "            for val in valids:\n",
    "                forbidden_ints.append(words_to_ind_dict[corpus[i+val]])\n",
    "            for val in valids:\n",
    "                n_sampled = 0\n",
    "                training_set.append((words_to_ind_dict[word], words_to_ind_dict[corpus[i+val]], 1.))\n",
    "                if abs(val) < 3:\n",
    "                    training_set.append((words_to_ind_dict[corpus[i+val]], words_to_ind_dict[word], 1.))\n",
    "\n",
    "                while n_sampled < 10:\n",
    "                    a = np.random.randint(0, dict_len)\n",
    "                    if a not in forbidden_ints:\n",
    "                        sampled_neg = a\n",
    "                        n_sampled += 1\n",
    "                        training_set.append((words_to_ind_dict[word], sampled_neg, 0.))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7124e58-b198-4db2-8159-6de636c17420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:29:37.062414Z",
     "iopub.status.busy": "2023-10-27T20:29:37.062220Z",
     "iopub.status.idle": "2023-10-27T20:29:37.071284Z",
     "shell.execute_reply": "2023-10-27T20:29:37.070357Z",
     "shell.execute_reply.started": "2023-10-27T20:29:37.062398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('house', 'saying', 0.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_words_dict[training_set[74512][0]], ind_to_words_dict[training_set[74512][1]], training_set[74512][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c1fe573-b77c-4860-8038-9c6ea079f7b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:29:37.072725Z",
     "iopub.status.busy": "2023-10-27T20:29:37.072524Z",
     "iopub.status.idle": "2023-10-27T20:29:37.095548Z",
     "shell.execute_reply": "2023-10-27T20:29:37.094923Z",
     "shell.execute_reply.started": "2023-10-27T20:29:37.072711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164541480"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc06c36b-2ab5-4d8a-af22-ee12ef4d266c",
   "metadata": {},
   "source": [
    "# Creating model and pytorch tensors for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "858dc8b1-64b5-459c-ab5c-709193b6bcd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:29:37.096670Z",
     "iopub.status.busy": "2023-10-27T20:29:37.096468Z",
     "iopub.status.idle": "2023-10-27T20:29:37.106183Z",
     "shell.execute_reply": "2023-10-27T20:29:37.105511Z",
     "shell.execute_reply.started": "2023-10-27T20:29:37.096657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "#(\n",
    "#    \"cuda:0\"\n",
    "#    if torch.cuda.is_available()\n",
    "#    else \"mps\"\n",
    "#    if torch.backends.mps.is_available()\n",
    "#    else \"cpu\"\n",
    "#)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d3a6d44-afe3-4092-ac0d-b1a4b08107c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:29:37.107379Z",
     "iopub.status.busy": "2023-10-27T20:29:37.107046Z",
     "iopub.status.idle": "2023-10-27T20:29:37.120688Z",
     "shell.execute_reply": "2023-10-27T20:29:37.119752Z",
     "shell.execute_reply.started": "2023-10-27T20:29:37.107360Z"
    }
   },
   "outputs": [],
   "source": [
    "class word2vec(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb_targ = torch.nn.Embedding(dict_len, 200)\n",
    "        self.emb_con = torch.nn.Embedding(dict_len, 200)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        target, context = x1, x2\n",
    "        word_emb = self.emb_targ(target.to(device)).unsqueeze(1)\n",
    "        context_emb = self.emb_con(context.to(device)).unsqueeze(2)\n",
    "        out = word_emb.bmm(context_emb)\n",
    "        output = torch.flatten(out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d30d9bf2-2c66-4b26-966d-d7ec289a27c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:29:37.122325Z",
     "iopub.status.busy": "2023-10-27T20:29:37.121830Z",
     "iopub.status.idle": "2023-10-27T20:29:37.132328Z",
     "shell.execute_reply": "2023-10-27T20:29:37.131533Z",
     "shell.execute_reply.started": "2023-10-27T20:29:37.122299Z"
    }
   },
   "outputs": [],
   "source": [
    "#training_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193845b-6724-48a4-aeab-26b9b116530f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:29:37.134116Z",
     "iopub.status.busy": "2023-10-27T20:29:37.133357Z"
    }
   },
   "outputs": [],
   "source": [
    "X1 = np.array(training_set)[:,0]\n",
    "X1 = [int(x) for x in X1]\n",
    "X1 = torch.tensor(X1, dtype=torch.int32).to(device)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "X2 = np.array(training_set)[:,1]\n",
    "X2 = [int(x) for x  in X2]\n",
    "X2 = torch.tensor(X2, dtype=torch.int32).to(device)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "Y = np.array(training_set)[:,2]\n",
    "Y = [y for y in Y]\n",
    "Y = torch.tensor(Y).to(device)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e81c22-695c-421c-95ec-14d86d0122f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_ = torch.randperm(len(X1))#.to(\"cuda\")  # Generate indices on GPU\n",
    "\n",
    "X1 = X1[indices_]\n",
    "X2 = X2[indices_]\n",
    "Y = Y[indices_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f20f0-89d2-49f6-9dbd-40061835426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_batches = torch.split(X1, 4096, dim=0)\n",
    "X2_batches = torch.split(X2, 4096, dim=0)\n",
    "Y_batches = torch.split(Y, 4096, dim=0)\n",
    "\n",
    "len(X1_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4703f87-ec51-4745-a09d-95c2c8f01836",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c1a22-67b8-4780-9888-20eaa7206a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_batches[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b971a7-5054-443e-a348-2695581ae245",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X1 = X1_batches[:-3000]\n",
    "train_X2 = X2_batches[:-3000]\n",
    "train_Y = Y_batches[:-3000]\n",
    "\n",
    "test_X1 = X1_batches[-3000:]\n",
    "test_X2 = X2_batches[-3000:]\n",
    "test_Y = Y_batches[-3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aae074-f603-4448-ba88-096e66106367",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_batches[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e453bd-8cb2-4fbc-8ab3-3f6ca7c4f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff7fab-e786-4e64-9702-7c4aad475b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = word2vec().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e57d2f-f95c-491a-b24a-eac12bfe7372",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb480f7d-8429-4d64-89ad-34aad19b1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(w2v.parameters(), lr=0.002)#, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28470d7-4bcb-4054-8544-c3add593a024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (x1, x2, y) in enumerate(zip(train_X1, train_X2, train_Y)):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #x1, x2, y = data\n",
    "        #print(x1)\n",
    "        #print(x2)\n",
    "        #print(y)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = w2v(x1.to(device), x2.to(device)).type(torch.float32)\n",
    "        #print(outputs.size())\n",
    "        #print(y.size())\n",
    "        loss = criterion(outputs, torch.flatten(y.to(device)).type(torch.float32))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statisticsq3\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            losses.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d38d6c-332f-4c6f-990b-f27cfd329ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4babf9-5159-4290-a0a0-88ffea3ef259",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf2c8fa-3032-4880-bd2d-4eea4f216ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X1_ = torch.cat(test_X1)\n",
    "test_X2_ = torch.cat(test_X2)\n",
    "test_Y_ = torch.cat(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20352936-19f7-4451-a288-8b99202d364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y_.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705732d-e403-45e8-8e97-02944cd2c685",
   "metadata": {},
   "source": [
    "## Testing on target-context pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f97e4-ee51-4951-bc39-d2845effc6b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for i, (x1, x2, y) in enumerate(zip(test_X1_[:1000000], test_X2_[:1000000], test_Y_[:1000000])):\n",
    "        if (i % 100000) == 0:\n",
    "            print(i)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = w2v(x1.unsqueeze(0),x2.unsqueeze(0))\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        predicted = (outputs > 0).float()\n",
    "        total += 1\n",
    "        #if (i % 1000) == 0:\n",
    "        #print((predicted == y).sum().item())\n",
    "        correct += (predicted == y).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0331e8a1-70e7-4ec7-aea4-eaacaa3ae3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce66fc-ccb7-4d3e-a80a-d8f005fd1410",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeds = w2v.emb_con.weight.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0524b40-a76b-4ef0-817a-9ee15203ace3",
   "metadata": {},
   "source": [
    "## Testing on common sense closest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb2272-34fb-45b1-96c0-8c27fe5e1179",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_ind_dict['green']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93414ba8-fe96-41ca-bbdd-98086d0eef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb31195d-c2ae-4b9c-be88-86ea9d6558a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeds[913]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7e7a20-f072-4bb4-a509-a918d9920390",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = list()\n",
    "val = Embeds[words_to_ind_dict[word1]]/abs(Embeds[words_to_ind_dict[word1]]).sum()\n",
    "for i,word2 in enumerate(list(words_to_ind_dict.keys())):\n",
    "    a = abs(Embeds[words_to_ind_dict[word2]]).sum()\n",
    "    if a != 0:\n",
    "        diff_vec = abs(val - Embeds[words_to_ind_dict[word2]]/a).sum()\n",
    "        diffs.append([i, diff_vec])\n",
    "best_ws = sorted(diffs, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3eb1a8-33cf-496d-bbaf-1187651c5fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[list(words_to_ind_dict.keys())[x[0]] for x in best_ws[0:6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62a4fe-e3d9-4d9d-a63b-e232b22528f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43b667-f25a-4556-84e4-b628e1f1f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_ = list(w2v.emb_targ.parameters())[0]\n",
    "embeddings_ = embeddings_.cpu().detach().numpy() \n",
    "norms = (embeddings_ ** 2).sum(axis=1) ** (1 / 2)\n",
    "norms = norms.reshape(norms.shape[0], 1)\n",
    "embeddings_ = embeddings_ / norms\n",
    "\n",
    "def get_similar_words(word, n):\n",
    "    word_id = words_to_ind_dict[word]\n",
    "    if word_id == 0:\n",
    "        print(\"Out of vocabulary word\")\n",
    "        return\n",
    "\n",
    "    word_vec = embeddings_[word_id]\n",
    "    word_vec = np.reshape(word_vec, (word_vec.shape[0], 1))\n",
    "    dists = np.matmul(embeddings_, word_vec).flatten()\n",
    "    topN_ids = np.argsort(-dists)[1 : n + 1]\n",
    "\n",
    "    topN_dict = {}\n",
    "    for sim_word_id in topN_ids:\n",
    "        sim_word = ind_to_words_dict[sim_word_id]\n",
    "        topN_dict[sim_word] = dists[sim_word_id]\n",
    "    return topN_dict\n",
    "\n",
    "\n",
    "get_similar_words('great', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162b3f7-cefe-4ea0-8268-bd3aeda3b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#((embeddings_[words_to_ind_dict['weak']] - embeddings_[words_to_ind_dict['strong']]) ** 2).sum() ** (1 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba15920-b893-479f-97cf-a09e5a5c50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = list()\n",
    "val = (embeddings_[words_to_ind_dict['weaker']] - embeddings_[words_to_ind_dict['weak']]) + embeddings_[words_to_ind_dict['strong']]\n",
    "\n",
    "for i,word2 in enumerate(list(words_to_ind_dict.keys())):\n",
    "    a = abs(Embeds[words_to_ind_dict[word2]]).sum()\n",
    "    if a != 0:\n",
    "        diff_vec = abs(val - Embeds[words_to_ind_dict[word2]]/a).sum()\n",
    "        diffs.append([i, diff_vec])\n",
    "best_ws = sorted(diffs, key=lambda x: x[1])\n",
    "\n",
    "[list(words_to_ind_dict.keys())[x[0]] for x in best_ws[0:6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640b818-9b66-4dbe-9eee-2cc8783eb886",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeds = pd.DataFrame(Embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ddb6a9-9b39-46bc-ad4f-dbb0eefc5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe8ec3-17e1-4a81-8bd9-5debeede455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict_df = pd.DataFrame([words_to_ind_dict.keys(), words_to_ind_dict.values()]).T[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7463cb-66de-4417-afa5-e8e2958cfc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#words_dict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b592678-3acf-4cdf-bc00-f9afc3b62d06",
   "metadata": {},
   "source": [
    "# Saving embeddings and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d791f-d712-4397-a49d-f9741a34c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savetxt\n",
    "\n",
    "Embeds.to_csv(\"Embeddings.csv\")\n",
    "words_dict_df.to_csv(\"words_dict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d1d7d7-92f9-4be5-a052-fce68b72c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv(\"Embeddings.csv\", index_col=0)\n",
    "#pd.read_csv(\"words_dict.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f97d6f-0bb7-4c3c-89f4-a5b64b2a3b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb28719-55fd-4a60-a5a3-f8dfc3fe34ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125bd7b-821b-4c9c-93cb-892dd572b4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba840a4-1021-4645-a3b0-569d70da1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CO_matrix_t(all_sentences):\n",
    "    window_size = 8\n",
    "    CO_matrix = np.zeros((dict_len,dict_len))\n",
    "    for all_words_ in all_sentences:\n",
    "        all_words_ = all_words_.split(' ')\n",
    "        for i in range(0, len(all_words_)):\n",
    "            if(all_words_[i] in words_to_ind_dict.keys()):\n",
    "                curr_word = all_words_[i]\n",
    "                for k in range(1, window_size+1):\n",
    "                    if((i + k) < len(all_words_)):\n",
    "                        if (all_words_[i+k] in words_to_ind_dict.keys()):\n",
    "                            ind_curr_word = words_to_ind_dict[curr_word]\n",
    "                            a = window_size+1 - k\n",
    "                            CO_matrix[words_to_ind_dict[all_words_[i+k]]][ind_curr_word] += a\n",
    "\n",
    "    return CO_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf6465-3197-4fa8-9262-42025a0fca03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5d7b0-4949-46e6-a7ff-2624741e2450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LN-kernel",
   "language": "python",
   "name": "ln-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
